#!/usr/bin/env python
# coding: utf-8

# # Practice 1
# 
# Create two DataFrames
# 
# Then merges these two DataFrames using the 'Name' column as the key, and outputs the following merged DataFrame which includes their Name, Age and Occupation:

# In[1]:


import pandas as pd

#Create a dictionary with some sample data
Info_data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'Dave','Eva','Kevin'],
    'Birth_Date': ['1991-12-01', '1980-01-11', '1987-01-01', '1966-01-18', '1996-10-10', '1999-11-11']}

Occupation_data = {
    'Name': ['Alice', 'Bob', 'Eve', 'Eva', 'Kevin'],
    'Occupation': ['Engineer', 'Doctor', 'Teacher', 'Student', 'Student']}


#Create a dataframe from the dictionary
Info_df = pd.DataFrame(Info_data)
Occupation_df = pd.DataFrame(Occupation_data)


# In[2]:


Info_df


# In[3]:


Occupation_df


# In[4]:


from datetime import date

def merging(df1, df2, key, how="inner"):
    merged_df = df1.merge(df2, on=key, how=how)
    
    # Calculate age and add it to the merged dataframe
    merged_df['Birth_Date'] = pd.to_datetime(merged_df['Birth_Date'])
    merged_df['Age'] = date.today().year - merged_df['Birth_Date'].dt.year
    merged_df = merged_df.drop(columns=['Birth_Date'])

    return merged_df


# If we want to merge and remain all values in Info_df

# In[5]:


merged_df = merging(Info_df, Occupation_df, key="Name", how="left")
merged_df


# Else If we want to merge and remove all rows contain null 

# In[6]:


merged_df = merging(Info_df, Occupation_df, key="Name", how="inner")
merged_df


# # Practice 2

# In[7]:


import pandas as pd

df = pd.DataFrame({'A': ['foo', 'bar', 'baz'],
                   'B': [[1, 2], [3, 4, 5], [6]]})
df


# In[8]:


df = df.explode(column='B')
df


# In[9]:


df.to_csv('outputs/practice2.csv', index=False)


# # Practice 3

# ## 1. Load the dataset & basic EDA

# In[10]:


df = pd.read_csv("dataset/practice3.csv")
df


# In[11]:


df.info()


# From df.info() we can see no missing values in the dataset

# In[12]:


# Further explore on statistical value of age, price and quantity
df.describe()


# ## 2. Total revenue 
# 

# In[13]:


total_revenue = (df['price'] * df['quantity']).sum()
print('Total revenue:', total_revenue)


# ## 3. Average price and quantity of clothing each category

# In[14]:


df.groupby('category').agg({'price': 'mean', 'quantity': 'mean'})


# ## 4. Group the dataset by the customer's age and gender and calculate the total revenue generated by each age/gender group.

# In[15]:


df['revenue'] = df.price*df.quantity
df.groupby(['gender', 'age'])['revenue'].sum()


# ## 5. Top 3 cities of total revenue and insights on the factors that contributed to their success.
# 

# In[16]:


df


# In[17]:


city_revenue_df = df.groupby('city').agg({'customer_id' : pd.Series.nunique,
                                       'age': 'mean',
                                       'category': list,
                                       'sub_category': list,
                                       'price': 'mean',
                                       'quantity': 'mean',
                                       'revenue': 'sum',
                                      })\
                .sort_values(by='revenue', ascending=False)

city_revenue_df


# In[18]:


print("Top 3 cities of total revenue:")
city_revenue_df['revenue'].head(3)


# From data explored on city_revenue_df, which each city has its mean age, mean price, mean quantity per customer, and their category purchased.
# 
# We can conclude that highest revenue due to customers purchasing high-priced items like Suits, Dresses, Jeans. 
# 
# Meanwhile, T-shirts is the lowest priced items and not contribute much to the revenue
# 
# Some other factors such as age, gender is not likely affect the revenue

# ## 6. Based on our analysis, we can provide the following recommendations to improve the online store's revenue and customer satisfaction:
# 
# Some of the ways the online store can improve its revenue and customer satisfaction based on the analysis are:
# 
# 1. Promote high-priced items: As seen in the analysis, high-priced items like suits and party dresses contributed to the store's revenue. Promoting these items can attract customers who are willing to pay a premium for quality clothing.
# 
# 2. Increase the quantity of items purchased: Customers purchasing multiple items in a single transaction can help increase the store's revenue. The online store can offer bundle deals or discounts for purchasing multiple items.
# 
# 3. Improve the selection of items: Offering a wide range of clothing items in different categories can attract customers with different preferences. The online store can analyze the current trends and demand and offer items accordingly.

# # Practice 4
# 
# Purchase frequency (average days between purchases of each customer)

# In[19]:


df = pd.read_csv("dataset/practice4.csv")
df


# In[20]:


# Format datetime column
df['date'] = pd.to_datetime(df['date'])

# Sort df by customer_id and date
df = df.sort_values(by=["customer_id", "date"]).reset_index(drop=True)

# Calculate interval days between purchased days for each customer
df['interval'] = df.groupby('customer_id')['date'].diff(periods=1).dt.days

# Calculate purchase frequency (mean interval days between 2 purchases)
purchase_frequency = df.groupby('customer_id')['interval'].mean()
purchase_frequency


# # Practice 5
# 
# Calculate the mean, median, and standard deviation of the daily revenue.

# In[21]:


df = pd.read_csv("dataset/practice5.csv")

# Format datetime column
df['date'] = pd.to_datetime(df['date'])
df


# In[22]:


# Quick code for showing basic stastics like mean, std, min, max and 25, 75% percentile
df.describe()


# In[23]:


print('Mean of daily revenue:', df.revenue.mean())
print('Median of daily revenue:', df.revenue.median())
print('Standard deviation of daily revenue:', df.revenue.std())


# # Practice 6
# 
# Analyze sales trends by product category, region, and month and make recommendations

# In[24]:


import random
import pandas as pd

# Define the product categories, regions, and months
product_categories = ['Electronics', 'Clothing']
regions = ['North', 'South']
months = ['Jan', 'Feb', 'Mar']

# Generate sales data for each product category, region, and month
sales_data = []
for product in product_categories:
    for region in regions:
        for month in months:
            sales = random.randint(5000, 15000)  # Generate random sales amount between 5000 and 15000 dollars
            sales_data.append([product, region, month, sales])

# Convert the sales data to a pandas DataFrame
columns = ['Product Category', 'Region', 'Month', 'Sales ($)']
sales_df = pd.DataFrame(sales_data, columns=columns)

# Converting Month to datetime
sales_df["Month"] = pd.to_datetime(sales_df["Month"], format="%b").dt.month
sales_df


# In[25]:


# Group data by Product Category, Region, and Month
sales_df = sales_df.groupby(["Product Category", "Region", 'Month'])["Sales ($)"].sum().reset_index()


# ## Exploratory Data Analysis

# In[26]:


import matplotlib.pyplot as plt


# Plot Line chart for each combination of ["Product Category", "Region"] by Month
plt.figure(figsize=(12, 6))
for category in sales_df["Product Category"].unique():
    for region in sales_df["Region"].unique():
        data = sales_df[(sales_df["Product Category"] == category) & (sales_df["Region"] == region)]
        plt.plot(data["Month"], data["Sales ($)"], label=f"{category} ({region})")
        
plt.legend()
plt.title("Sales Trends by Product Category and Region")
plt.xlabel("Month")
plt.ylabel("Sales ($)")
plt.show()


# ## Two-way ANOVA test
# 
# Perform two-way ANOVA  with two categorical independent variables, "Product Category" and "Region," and one continuous dependent variable: Month
# 
# 

# In[27]:


import statsmodels.api as sm
from statsmodels.formula.api import ols

# Perform two-way ANOVA  with two categorical independent variables, "Product Category" and "Region," 
# and one continuous dependent variable: Month
model = ols("Q('Sales ($)') ~ C(Q('Product Category')) + C(Region)", data=sales_df).fit()
anova_table = sm.stats.anova_lm(model, typ=2)
print(anova_table)


# The "sum_sq" column represents the sum of squares, which is a measure of the variation in the data that can be attributed to each source of variation.
# 
# The "df" column represents the degrees of freedom for each source of variation, which is the number of independent observations minus the number of parameters estimated from the data.
# 
# The "F" column represents the F-statistic, which is the ratio of the variation between groups to the variation within groups.
# 
# The "PR(>F)" column represents the p-value, which is the probability of observing a F-statistic as large or larger than the observed value if the null hypothesis is true.
# 
# In this table, the null hypothesis for each factor is that there is no significant effect on the dependent variable. The p-values for both factors are greater than the significance level of 0.05, indicating that there is not enough evidence to reject the null hypothesis. Therefore, it can be concluded that there is no significant interaction between "Product Category" and "Region" and neither "Product Category" nor "Region" has a significant main effect on the dependent variable. The majority of the variation in the dependent variable is due to the residual term, which represents unexplained variation or random error.
# 
# 
# 
# 

# **Recommendations:** Based on the analysis, we can make recommendations to improve sales. For example, if we identify a seasonal pattern in sales, we can adjust our marketing and promotional activities accordingly. If we find that certain product categories or regions are performing better than others, we can focus our resources on those areas. Additionally, if we find that there are significant differences in sales between different product categories and regions, we can adjust our pricing or product offerings to better match the preferences of our customers in those areas.
# 
# 
# 

# # Practice 7
# 
# Analyze customer purchasing behavior

# In[28]:


df = pd.read_csv('dataset/customer_purchases.csv')
df


# ## 1. Average purchase amount for each product category

# In[29]:


df.groupby('Product Category')['Purchase Amount ($)'].mean()


# ## 2. Average age and income of customers who make purchases in each product category

# In[30]:


# Drop duplicates ['Customer ID', 'Product Category'] for make sure each customer for each product appears once
df.drop_duplicates(['Customer ID', 'Product Category']).groupby('Product Category')[['Age', 'Income ($)']].mean()


# ## 3. Gender distribution of customers in each product category

# In[31]:


gender_distribution = df.drop_duplicates(['Customer ID', 'Product Category'])                        .groupby('Product Category').agg({'Gender': 'value_counts'})                        .rename({'Gender': 'count'}, axis=1)                        .reset_index(drop=False)
gender_distribution


# In[32]:


# Pivot the DataFrame to get the desired format for plotting
gender_distribution = gender_distribution.pivot(index='Product Category', columns='Gender', values='count')
gender_distribution


# In[33]:


ax = gender_distribution.plot(kind='bar', rot=0, stacked=False)
ax.set_xlabel('Product Category')
ax.set_ylabel('Count')
ax.set_title('Gender Distribution of Customers by Product Category')
plt.show()


# ## 4. Check if any customers make multiple purchases for same product category 
# 
# Average time between purchases and average purchase amount?
# 

# In[34]:


# Check duplicates of 'Customer ID' and 'Product Category'
duplicates = df.duplicated(['Customer ID', 'Product Category'], keep=False)

# Get all multiple purchases by duplicates mask
multiple_purchases = df[duplicates].copy()
multiple_purchases


# In[35]:


# Calculate the interval time between purchases for each customer/product category combination
multiple_purchases['interval_time'] = multiple_purchases.groupby(['Customer ID', 'Product Category'])['Purchase Date'].diff(periods=1)

# Calculate the average time between purchases and average purchase amount for each customer/product category combination
multiple_purchases = multiple_purchases.groupby(['Customer ID', 'Product Category']).agg({'interval_time': 'mean', 'Purchase Amount ($)': 'mean'})
multiple_purchases


# ## 5. Average purchase amount each month & Seasonal trends in purchasing behavior?
# 

# In[36]:


# Convert the Purchase Date column to a datetime data type
# Note that because SQL Server, the default zero date is January 1, 1900
# So I assume Purchase Date is number of dates since this day  
df['Purchase Date'] = pd.to_datetime(df['Purchase Date'], unit='D', origin='1900-01-01')

# Add a Month column
df['Month'] = df['Purchase Date'].dt.month
df


# In[37]:


# Group the data by month and calculate the average purchase amount for each month
avg_month_purchase = df.groupby('Month')['Purchase Amount ($)'].mean()
avg_month_purchase


# In[38]:


avg_month_purchase.plot.line(title='Average purchase amount each month')


# From the above chart, purchase in Feb is the highest, compare to Jan and Mar. 
# 
# However, the provided data of average purchase each month is not sufficient to make a conclusion about seasonal trends as it only provides information about three months. 
# 
# We need more data, preferably for a longer time period, to analyze seasonal trends in purchasing behavior.
# 
# 

# # Practice 8
# 
# Categorize Age to Young (<30), Middle (30-60), and Old (>60)

# In[39]:


import pandas as pd

# Create a dictionary of data
data = {'Name': ['John', 'Jane', 'Bob', 'Alice'],
        'Age': [25, 30, 27, 40]}

# Create a DataFrame from the dictionary
df = pd.DataFrame(data)

# Display the DataFrame
df


# In[40]:


def categorize_age(age):
    if age<30:
        return 'Young'
    elif age<=60:
        return 'Middle'
    else:
        return 'Old'

df['Category'] = df.Age.apply(categorize_age)
df


# In[ ]:




